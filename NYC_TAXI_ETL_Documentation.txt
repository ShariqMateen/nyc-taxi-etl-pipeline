Project Documentation - NYC_TAXI_ETL

Project Structure Overview
==========================

NYC_TAXI_ETL/
│
├── airflow_project/
│   ├── dags/
│   │   ├── etl_pipeline.py
│   │   └── extract.py
│   |    ├── Transform.py
│   |    ├── Load.py
│   ├── logs/
│   ├── plugins/
│   └── docker-compose.yaml
│
├── data/
│   ├── data_clean/ 
│   │   Contains cleaned parquet files produced after running transform.py.
│   │
│   ├── data_csv/
│   │   Contains CSV versions of the cleaned parquet files.
│   │
│   └── data_raw/
│       Contains raw parquet files downloaded from AWS S3.
│
├── NYC-Database-creation/
│   └── NYC-Data-table-queries.sql
│       Contains SQL schema creation queries for trips_raw and trips_clean tables.
│
├── scripts/
│   ├── ETL_Logs/
│   │   Contains log files generated by extract, transform, and load scripts.
│   │
│   ├── extract.py
│   ├── Transform.py
│   ├── Load.py
│   └── helping.py
│
├── Visualization_jupyterNB/
│   ├── .ipynb_checkpoints/
│   └── Analysis.ipynb
│       Notebook used for analysis and visualization of the cleaned dataset.
│
└── requirement.txt


Detailed Component Descriptions
===============================

1. airflow_project/  "If you want to run ETL using airflow then use ETL code of this folder, otherwise use ETL code of scripts folder (if you want to run locally)
   ----------------
   Contains everything needed to orchestrate this ETL pipeline in Apache Airflow:
   - dags/: Contains DAG definitions including the main ETL pipeline.
   - logs/: Stores execution logs when Airflow runs tasks.
   - plugins/: Custom operators/hooks if needed.
   - docker-compose.yaml: Airflow Docker setup for local execution.



2. data/
   ------
   • data_raw/: Raw parquet files downloaded directly from the NYC TLC public S3 bucket.
   • data_clean/: Cleaned files created after applying transformations such as:
       - Type conversions
       - Removing invalid rows
       - Standardizing columns
   • data_csv/: Cleaned files converted into CSV format for easier visualization and external tools.

3. NYC-Database-creation/
   -----------------------
   Contains SQL file defining schema and table creation for:
   - trips_raw
   - trips_clean
	
	When you create databsase in postgressql database then you execute all these queries one by one along with alter queries. "SELECT" query is used to check data of table.

4. scripts/
   ---------
   Core Python ETL scripts executed locally (if you want to run code locally (Manually) then use this ETL code
   - extract.py: 
	This script performs the Extract step of the ETL pipeline by downloading raw NYC Taxi data and validating it before transformation.
		Downloads 6 months (Jan–Jun 2023) of NYC Yellow Taxi data in Parquet format from the official TLC public cloud URL.

		Saves all downloaded files into:
		data/data_raw/

		Uses streaming download, ensuring large files are saved safely without memory issues.

		After downloading, the script verifies each Parquet file by:

		Checking file size (MB)
		
		Reading the file to count total records

		All verification results are logged into:
		scripts/ETL_Logs/extract_log.log

		Handles read/download errors gracefully and logs failures.

		Designed to be run directly.

  - Transform.py :

This script performs the Transform step of the ETL pipeline. It reads raw Parquet files, cleans the data, creates new features, and saves cleaned outputs.
Key Functions & Responsibilities

1. Reads Raw Parquet Files

Scans data/data_raw/
Loads each .parquet file using PyArrow and Pandas
Processes every month’s file one by one

2. Cleans and Validates Data
The clean_dataframe() function performs multiple cleaning steps:

Fixes data types for timestamps, IDs, and categorical fields
Handles missing values in store_and_fwd_flag and converts it to boolean
Removes invalid rows, such as:
	Dropoff earlier than pickup
	Zero or negative passenger counts
	Zero or negative trip distance
	Invalid payment types

Validates fare structure:
Recalculates expected total fare and removes rows where mismatch > $1

Adds useful derived columns:
	
	trip_duration_minutes
	average_speed_mph
	pickup_date, pickup_hour, pickup_weekday

Drops the helper calculation columns after validation

3. Saves Cleaned Output

After cleaning each file:

Writes a clean Parquet file into: 
	data/data_clean/clean_<original_file>.parquet

Converts the same dataframe to CSV and saves it into:
	data/data_csv/<file>.csv

4. Logs Processing Details

Writes detailed logs into: scripts/ETL_Logs/transform_log.log

Each log entry contains:

Number of cleaned rows
Time taken to convert CSV
Parquet vs CSV file sizes
Size differences (helpful for debugging and storage planning)


 - Load.py
This script performs the Load step of the ETL pipeline. It takes both raw and cleaned datasets and loads them into PostgreSQL tables (trips_raw and trips_clean) using fast bulk-loading.
Key Responsibilities

1. Connects to PostgreSQL

Establishes a database connection using psycopg2
Connection details are set inside the get_connection() function

2. Loads Cleaned Data into trips_clean Table and Loads Raw Data into trips_raw Table

Reads each cleaned Parquet file from: data/data_clean/
AND
Reads each raw Parquet file from: data/data_raw/
Converts the Parquet file into a Pandas DataFrame
Ensures None is used for missing values
Orders columns exactly as required by the database schema
Uses PostgreSQL COPY FROM (via copy_to_postgres_clean()) for high-speed insertion
Inserts all rows of cleaned data into the trips_clean table
Inserts all rows of raw data into the trips_raw table
Writes detailed logs such as:
	File name
	Number of rows inserted
	Any errors during insertion

3. Ensures FAST Database Loading

Both loaders use:

✔ COPY FROM STDIN
✔ A memory-based buffer (io.StringIO)
✔ TSV format (tab-separated)
✔ Proper handling of null values (\N)

This makes ingestion extremely fast even for millions of rows.

4. Logging

All load operations write logs into:
 		scripts/ETL_Logs/data_load.log

Logs include:
Start and end of load process
Files processed
Number of inserted rows
Database errors + rollback status

Configuration Required Before Running the Load Step
Before running the load.py, you must update the database connection settings to match your PostgreSQL environment.
These parameters are located inside the script where the psycopg2.connect() function is called.

conn = psycopg2.connect(
    dbname="YOUR_DATABASE_NAME",
    user="YOUR_USERNAME",
    password="YOUR_PASSWORD",
    host="YOUR_HOST",
    port="YOUR_PORT"
)

you must replace the placeholder values with your own.
And also Ensure Required Tables(trips_raw , trips_clean) Exist
If they don’t exist, then you needs to run the provided SQL table creation script.

   - helping.py → used to get specific data or information.

   ETL_Logs/: Generated logs for traceability and debugging.

5. Visualization_jupyterNB/
   -------------------------
Analysis Summary & Insights (from Analysis.ipynb)

Below is a structured explanation of all the analysis performed insights produce.

1️⃣ Trip Volume by Hour of Day

What was done:
A line plot was generated showing the number of taxi trips for each hour (0–23).

Insights:

Clear peak hours are visible (usually morning commute, evening rush, and late-night activity).
Helps identify high-demand hours for taxi services.

2️⃣ Heatmap: Trips by Day of Week vs Hour of Day

What was done:

A heatmap shows trip density across weekday (Mon–Sun) and hour of day.

Insights:

Shows behavior patterns, such as:
   	Fridays/Saturdays having heavier late-night traffic.
	Weekday peaks during commute hours.

Excellent for operational planning, staffing, or surge pricing.

3️⃣ Average Trip Duration & Distance by Hour

What was done:
Two line charts comparing:
	Average trip duration per hour
	Average trip distance per hour

Insights:
Duration increases during rush hours (traffic).
Distance may stay stable or drop depending on short-trip patterns during rush periods.
Useful for understanding traffic impact.

4️⃣ Most Popular Pickup Zones for a Specific Hour

What was done:
The dataset groups trips by PULocationID for a chosen hour (example: hour 17).
A chart highlights the top 10 busiest pickup zones.

Insights:
Shows location hotspots at any time of the day.
Critical for taxi positioning, demand prediction, and urban mobility analysis.

5️⃣ Monthly Trend Analysis (Trips & Fare)

What was done:

A month-wise analysis that includes:
	Total trips per month
	Total fare collected per month

Two trend lines plotted together.

Insights:

Shows seasonal variations (peaks, dips).
Helps the client spot:
	High revenue months
	Seasonal demand patterns

6️⃣ KPI Computation Section
This section calculates key business metrics, including:
Average trips per hour
Average fare per trip
Month-over-month percent changes
KPI dictionary stored for reporting dashboards

Insights:
Gives a quick snapshot of performance.
Helps track business growth or decline.

6. requirement.txt
   ----------------
   Contains all Python dependencies required to run ETL + visualization. requirements.txt lists all Python libraries your NYC Taxi ETL project needs to run.
	pip install -r requirements.txt
   First of all run this above command in your vs-code or any project terminal, it install all necessary libraries that is used to run your ETL script.










✅ How to Run the ETL Pipeline Using Apache Airflow 

Follow the steps below to run the full ETL workflow using Airflow and Docker.

1️⃣ Install Docker Desktop

Airflow in this project runs inside Docker containers, so first:

Download Docker Desktop (Windows/Mac/Linux).

Install it normally and make sure Docker Engine is running.

2️⃣ Open the Project Folder

Open VS Code (or any terminal you prefer).

Open the project directory:

airflow_project/


This folder contains:

docker-compose.yml

DAGs (dags/)

Scripts (scripts/)

ETL code

3️⃣ Install & Initialize Airflow Environment

Inside the terminal, navigate into the project folder:

cd airflow_project


Run Airflow initialization:

docker compose up airflow-init


This command creates the metadata database, users, and required folders.

4️⃣ Start Airflow Services

Run the full Airflow environment:

docker compose up -d


This starts:

Airflow Webserver

Scheduler

Postgres

Redis

Workers

All in background mode (-d).

5️⃣ Open Airflow UI

After ~15–30 seconds, open your browser and go to:

http://localhost:8080


Login with the default credentials:

Username: airflow

Password: airflow

(You can change this later if you wish.)

6️⃣ Run the ETL DAG

In the Airflow UI, go to the DAGs tab.

Look for your DAG name (e.g., nyc_etl_pipeline).

Toggle it ON.

Click the ▶️ Run button to start the ETL pipeline.

Airflow will now:

Extract → Transform → Load

Download raw taxi data

Clean and validate the data

Save clean & CSV versions

Load both raw and clean data into PostgreSQL

Logs for every task are available inside the Airflow UI.

7️⃣ (Optional) Stop Airflow Containers

When you're done:

docker compose down


